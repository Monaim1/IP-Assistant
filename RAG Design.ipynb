{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3cc926c",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "A Patent RAG system with:\n",
    "\n",
    "- Contextual Embeddings: chunks are enriched with concise context (e.g., “This chunk is from Patent X, abstract section, about photovoltaic cells, filed 2016-01-15”).\n",
    "\n",
    "- Hybrid Retrieval: combine vector similarity (Milvus) + lexical BM25 (via Elasticsearch/OpenSearch).\n",
    "\n",
    "- Re-ranking: re-order retrieved chunks with a cross-encoder or LLM scoring step.\n",
    "\n",
    "\n",
    "Contextual Retrieval is an advanced retrieval method proposed by Anthropic to address the issue of semantic isolation of chunks, which arises in current Retrieval-Augmented Generation (RAG) solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aceca61",
   "metadata": {},
   "source": [
    "## High-level System Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5ee39",
   "metadata": {},
   "source": [
    "1. Data ingestion & normalization\n",
    "\n",
    "        Source: HUPD dataset (abstract, claims, summary, full_description, metadata fields).\n",
    "\n",
    "        Normalize each record into structured JSON.\n",
    "\n",
    "        Store metadata (application number, filing date, CPC labels, etc.).\n",
    "\n",
    "2. Chunking\n",
    "\n",
    "        Chunk by semantic sections (abstract, claims, background, summary, full_description).\n",
    "\n",
    "        Within each, split into ~300–500 tokens (with overlap ~50 tokens).\n",
    "\n",
    "        Preserve section + patent metadata with each chunk.\n",
    "\n",
    "3. Contextualization (Anthropic method)\n",
    "\n",
    "        For each chunk:\n",
    "\n",
    "            Input to Claude (or another LLM): whole patent + chunk.\n",
    "\n",
    "            Ask for 50–100 token contextual summary situating chunk in the whole patent (which part, what invention, etc.).\n",
    "\n",
    "            Prepend this context to chunk text → \"contextualized chunk\".\n",
    "\n",
    "        Example:\n",
    "\n",
    "            Context: This chunk is from Patent 20160012345, Abstract, about solar panel efficiency improvements.  \n",
    "            Chunk: \"The system improves photon capture by embedding nanostructures in the substrate layer.\"\n",
    "\n",
    "4. Dual Indexing\n",
    "\n",
    "        Vector index (Milvus):\n",
    "\n",
    "        Store embeddings of contextualized chunks.\n",
    "\n",
    "        Embedding model: sentence-transformers or OpenAI/Anthropic embeddings.\n",
    "\n",
    "        BM25 index (ElasticSearch/OpenSearch):\n",
    "\n",
    "        Index contextualized chunk text (to catch exact terms, e.g. “US20160234A1”).\n",
    "\n",
    "5. Retrieval Pipeline\n",
    "\n",
    "        User query comes in.\n",
    "\n",
    "        Run query against Milvus (vector search) and BM25 (keyword search).\n",
    "\n",
    "        Merge candidate sets (e.g., 20 from each).\n",
    "\n",
    "        Deduplicate.\n",
    "\n",
    "        Re-rank using:\n",
    "\n",
    "            Cross-encoder (e.g. ms-marco-MiniLM-L-6-v2)\n",
    "\n",
    "            OR a lightweight LLM call scoring relevance.\n",
    "\n",
    "            Take top-K (e.g., 5–10 chunks).\n",
    "\n",
    "6. Answer generation\n",
    "\n",
    "        Construct prompt with:\n",
    "\n",
    "        User query\n",
    "\n",
    "        Retrieved contextualized chunks (as citations)\n",
    "\n",
    "        Send to LLM (Claude / GPT).\n",
    "\n",
    "        Ask for structured output (answer + cited patent IDs/chunks).\n",
    "\n",
    "7. Observability / monitoring\n",
    "\n",
    "        Log query latency, recall @K, and re-ranker confidence.\n",
    "\n",
    "        Track cost (embedding, contextualization).\n",
    "\n",
    "        Monitor cluster health (Milvus, ES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d69678",
   "metadata": {},
   "outputs": [],
   "source": [
    "### It seems like the HF repo is not working, i eded up downloading it manually\n",
    "# The dataset is mainly 2018 IP data from HUPD \n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json\n",
    "\n",
    "# dataset_dict = load_dataset('HUPD/hupd',\n",
    "#     name='sample',\n",
    "#     data_files=\"https://huggingface.co/datasets/HUPD/hupd/blob/main/hupd_metadata_2022-02-22.feather\",\n",
    "#     icpr_label=None,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eac6204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['application_number', 'publication_number', 'title', 'decision', 'date_produced', 'date_published', 'main_cpc_label', 'cpc_labels', 'main_ipcr_label', 'ipcr_labels', 'patent_number', 'filing_date', 'patent_issue_date', 'abandon_date', 'uspc_class', 'uspc_subclass', 'examiner_id', 'examiner_name_last', 'examiner_name_first', 'examiner_name_middle', 'inventor_list', 'abstract', 'claims', 'background', 'summary', 'full_description'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_limit = 20\n",
    "ip_files = [json.load(open(\"RawData/2018/\" + file)) for file in os.listdir(r\"RawData/2018\")[:ip_limit]] \n",
    "ip_files[0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bea3a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "from typing import Callable\n",
    "from pymilvus import (\n",
    "    MilvusClient,\n",
    "    DataType,\n",
    "    AnnSearchRequest,\n",
    "    RRFRanker,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "class MilvusContextualRetriever:\n",
    "    def __init__(\n",
    "        self,\n",
    "        uri=\"milvus.db\",\n",
    "        collection_name=\"contexual_bgem3\",\n",
    "        dense_embedding_function=None,\n",
    "        use_sparse=False,\n",
    "        sparse_embedding_function=None,\n",
    "        use_contextualize_embedding=False,\n",
    "        anthropic_client=None,\n",
    "        use_reranker=False,\n",
    "        rerank_function=None,\n",
    "    ):\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        # For Milvus-lite, uri is a local path like \"./milvus.db\"\n",
    "        # For Milvus standalone service, uri is like \"http://localhost:19530\"\n",
    "        # For Zilliz Clond, please set `uri` and `token`, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.\n",
    "        self.client = MilvusClient(uri)\n",
    "\n",
    "        self.embedding_function = dense_embedding_function\n",
    "\n",
    "        self.use_sparse = use_sparse\n",
    "        self.sparse_embedding_function = None\n",
    "\n",
    "        self.use_contextualize_embedding = use_contextualize_embedding\n",
    "        self.anthropic_client = anthropic_client\n",
    "\n",
    "        self.use_reranker = use_reranker\n",
    "        self.rerank_function = rerank_function\n",
    "\n",
    "        if use_sparse is True and sparse_embedding_function:\n",
    "            self.sparse_embedding_function = sparse_embedding_function\n",
    "        elif sparse_embedding_function is False:\n",
    "            raise ValueError(\n",
    "                \"Sparse embedding function cannot be None if use_sparse is False\"\n",
    "            )\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def build_collection(self):\n",
    "        schema = self.client.create_schema(\n",
    "            auto_id=True,\n",
    "            enable_dynamic_field=True,\n",
    "        )\n",
    "        schema.add_field(field_name=\"pk\", datatype=DataType.INT64, is_primary=True)\n",
    "        schema.add_field(\n",
    "            field_name=\"dense_vector\",\n",
    "            datatype=DataType.FLOAT_VECTOR,\n",
    "            dim=self.embedding_function.dim,\n",
    "        )\n",
    "        if self.use_sparse is True:\n",
    "            schema.add_field(\n",
    "                field_name=\"sparse_vector\", datatype=DataType.SPARSE_FLOAT_VECTOR\n",
    "            )\n",
    "\n",
    "        index_params = self.client.prepare_index_params()\n",
    "        index_params.add_index(\n",
    "            field_name=\"dense_vector\", index_type=\"FLAT\", metric_type=\"IP\"\n",
    "        )\n",
    "        if self.use_sparse is True:\n",
    "            index_params.add_index(\n",
    "                field_name=\"sparse_vector\",\n",
    "                index_type=\"SPARSE_INVERTED_INDEX\",\n",
    "                metric_type=\"IP\",\n",
    "            )\n",
    "\n",
    "        self.client.create_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            schema=schema,\n",
    "            index_params=index_params,\n",
    "            enable_dynamic_field=True,\n",
    "        )\n",
    "\n",
    "    def insert_data(self, chunk, metadata):\n",
    "        dense_vec = self.embedding_function([chunk])[0]\n",
    "        if self.use_sparse is True:\n",
    "            sparse_result = self.sparse_embedding_function.encode_documents([chunk])\n",
    "            if type(sparse_result) == dict:\n",
    "                sparse_vec = sparse_result[\"sparse\"][[0]]\n",
    "            else:\n",
    "                sparse_vec = sparse_result[[0]]\n",
    "            self.client.insert(\n",
    "                collection_name=self.collection_name,\n",
    "                data={\n",
    "                    \"dense_vector\": dense_vec,\n",
    "                    \"sparse_vector\": sparse_vec,\n",
    "                    **metadata,\n",
    "                },\n",
    "            )\n",
    "        else:\n",
    "            self.client.insert(\n",
    "                collection_name=self.collection_name,\n",
    "                data={\"dense_vector\": dense_vec, **metadata},\n",
    "            )\n",
    "\n",
    "    def insert_contextualized_data(self, doc, chunk, metadata):\n",
    "        contextualized_text, usage = self.situate_context(doc, chunk)\n",
    "        metadata[\"context\"] = contextualized_text\n",
    "        text_to_embed = f\"{chunk}\\n\\n{contextualized_text}\"\n",
    "        dense_vec = self.embedding_function([text_to_embed])[0]\n",
    "        if self.use_sparse is True:\n",
    "            sparse_vec = self.sparse_embedding_function.encode_documents(\n",
    "                [text_to_embed]\n",
    "            )[\"sparse\"][[0]]\n",
    "            self.client.insert(\n",
    "                collection_name=self.collection_name,\n",
    "                data={\n",
    "                    \"dense_vector\": dense_vec,\n",
    "                    \"sparse_vector\": sparse_vec,\n",
    "                    **metadata,\n",
    "                },\n",
    "            )\n",
    "        else:\n",
    "            self.client.insert(\n",
    "                collection_name=self.collection_name,\n",
    "                data={\"dense_vector\": dense_vec, **metadata},\n",
    "            )\n",
    "\n",
    "    def situate_context(self, doc: str, chunk: str):\n",
    "        DOCUMENT_CONTEXT_PROMPT = \"\"\"\n",
    "        <document>\n",
    "        {doc_content}\n",
    "        </document>\n",
    "        \"\"\"\n",
    "\n",
    "        CHUNK_CONTEXT_PROMPT = \"\"\"\n",
    "        Here is the chunk we want to situate within the whole document\n",
    "        <chunk>\n",
    "        {chunk_content}\n",
    "        </chunk>\n",
    "\n",
    "        Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "        Answer only with the succinct context and nothing else.\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.anthropic_client.beta.prompt_caching.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=1000,\n",
    "            temperature=0.0,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": DOCUMENT_CONTEXT_PROMPT.format(doc_content=doc),\n",
    "                            \"cache_control\": {\n",
    "                                \"type\": \"ephemeral\"\n",
    "                            },  # we will make use of prompt caching for the full documents\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": CHUNK_CONTEXT_PROMPT.format(chunk_content=chunk),\n",
    "                        },\n",
    "                    ],\n",
    "                },\n",
    "            ],\n",
    "            extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"},\n",
    "        )\n",
    "        return response.content[0].text, response.usage\n",
    "\n",
    "    def search(self, query: str, k: int = 20) -> List[Dict[str, Any]]:\n",
    "        dense_vec = self.embedding_function([query])[0]\n",
    "        if self.use_sparse is True:\n",
    "            sparse_vec = self.sparse_embedding_function.encode_queries([query])[\n",
    "                \"sparse\"\n",
    "            ][[0]]\n",
    "\n",
    "        req_list = []\n",
    "        if self.use_reranker:\n",
    "            k = k * 10\n",
    "        if self.use_sparse is True:\n",
    "            req_list = []\n",
    "            dense_search_param = {\n",
    "                \"data\": [dense_vec],\n",
    "                \"anns_field\": \"dense_vector\",\n",
    "                \"param\": {\"metric_type\": \"IP\"},\n",
    "                \"limit\": k * 2,\n",
    "            }\n",
    "            dense_req = AnnSearchRequest(**dense_search_param)\n",
    "            req_list.append(dense_req)\n",
    "\n",
    "            sparse_search_param = {\n",
    "                \"data\": [sparse_vec],\n",
    "                \"anns_field\": \"sparse_vector\",\n",
    "                \"param\": {\"metric_type\": \"IP\"},\n",
    "                \"limit\": k * 2,\n",
    "            }\n",
    "            sparse_req = AnnSearchRequest(**sparse_search_param)\n",
    "\n",
    "            req_list.append(sparse_req)\n",
    "\n",
    "            docs = self.client.hybrid_search(\n",
    "                self.collection_name,\n",
    "                req_list,\n",
    "                RRFRanker(),\n",
    "                k,\n",
    "                output_fields=[\n",
    "                    \"content\",\n",
    "                    \"original_uuid\",\n",
    "                    \"doc_id\",\n",
    "                    \"chunk_id\",\n",
    "                    \"original_index\",\n",
    "                    \"context\",\n",
    "                ],\n",
    "            )\n",
    "        else:\n",
    "            docs = self.client.search(\n",
    "                self.collection_name,\n",
    "                data=[dense_vec],\n",
    "                anns_field=\"dense_vector\",\n",
    "                limit=k,\n",
    "                output_fields=[\n",
    "                    \"content\",\n",
    "                    \"original_uuid\",\n",
    "                    \"doc_id\",\n",
    "                    \"chunk_id\",\n",
    "                    \"original_index\",\n",
    "                    \"context\",\n",
    "                ],\n",
    "            )\n",
    "        if self.use_reranker and self.use_contextualize_embedding:\n",
    "            reranked_texts = []\n",
    "            reranked_docs = []\n",
    "            for i in range(k):\n",
    "                if self.use_contextualize_embedding:\n",
    "                    reranked_texts.append(\n",
    "                        f\"{docs[0][i]['entity']['content']}\\n\\n{docs[0][i]['entity']['context']}\"\n",
    "                    )\n",
    "                else:\n",
    "                    reranked_texts.append(f\"{docs[0][i]['entity']['content']}\")\n",
    "            results = self.rerank_function(query, reranked_texts)\n",
    "            for result in results:\n",
    "                reranked_docs.append(docs[0][result.index])\n",
    "            docs[0] = reranked_docs\n",
    "        return docs\n",
    "\n",
    "\n",
    "def evaluate_retrieval(\n",
    "    queries: List[Dict[str, Any]], retrieval_function: Callable, db, k: int = 20\n",
    ") -> Dict[str, float]:\n",
    "    total_score = 0\n",
    "    total_queries = len(queries)\n",
    "    for query_item in tqdm(queries, desc=\"Evaluating retrieval\"):\n",
    "        query = query_item[\"query\"]\n",
    "        golden_chunk_uuids = query_item[\"golden_chunk_uuids\"]\n",
    "\n",
    "        # Find all golden chunk contents\n",
    "        golden_contents = []\n",
    "        for doc_uuid, chunk_index in golden_chunk_uuids:\n",
    "            golden_doc = next(\n",
    "                (\n",
    "                    doc\n",
    "                    for doc in query_item[\"golden_documents\"]\n",
    "                    if doc[\"uuid\"] == doc_uuid\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "            if not golden_doc:\n",
    "                print(f\"Warning: Golden document not found for UUID {doc_uuid}\")\n",
    "                continue\n",
    "\n",
    "            golden_chunk = next(\n",
    "                (\n",
    "                    chunk\n",
    "                    for chunk in golden_doc[\"chunks\"]\n",
    "                    if chunk[\"index\"] == chunk_index\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "            if not golden_chunk:\n",
    "                print(\n",
    "                    f\"Warning: Golden chunk not found for index {chunk_index} in document {doc_uuid}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            golden_contents.append(golden_chunk[\"content\"].strip())\n",
    "\n",
    "        if not golden_contents:\n",
    "            print(f\"Warning: No golden contents found for query: {query}\")\n",
    "            continue\n",
    "\n",
    "        retrieved_docs = retrieval_function(query, db, k=k)\n",
    "\n",
    "        # Count how many golden chunks are in the top k retrieved documents\n",
    "        chunks_found = 0\n",
    "        for golden_content in golden_contents:\n",
    "            for doc in retrieved_docs[0][:k]:\n",
    "                retrieved_content = doc[\"entity\"][\"content\"].strip()\n",
    "                if retrieved_content == golden_content:\n",
    "                    chunks_found += 1\n",
    "                    break\n",
    "\n",
    "        query_score = chunks_found / len(golden_contents)\n",
    "        total_score += query_score\n",
    "\n",
    "    average_score = total_score / total_queries\n",
    "    pass_at_n = average_score * 100\n",
    "    return {\n",
    "        \"pass_at_n\": pass_at_n,\n",
    "        \"average_score\": average_score,\n",
    "        \"total_queries\": total_queries,\n",
    "    }\n",
    "\n",
    "\n",
    "def retrieve_base(query: str, db, k: int = 20) -> List[Dict[str, Any]]:\n",
    "    return db.search(query, k=k)\n",
    "\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load JSONL file and return a list of dictionaries.\"\"\"\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return [json.loads(line) for line in file]\n",
    "\n",
    "\n",
    "def evaluate_db(db, original_jsonl_path: str, k):\n",
    "    # Load the original JSONL data for queries and ground truth\n",
    "    original_data = load_jsonl(original_jsonl_path)\n",
    "\n",
    "    # Evaluate retrieval\n",
    "    results = evaluate_retrieval(original_data, retrieve_base, db, k)\n",
    "    print(f\"Pass@{k}: {results['pass_at_n']:.2f}%\")\n",
    "    print(f\"Total Score: {results['average_score']}\")\n",
    "    print(f\"Total queries: {results['total_queries']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5adab3b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VoyageEmbeddingFunction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m dense_ef = \u001b[43mVoyageEmbeddingFunction\u001b[49m(api_key=\u001b[33m\"\u001b[39m\u001b[33myour-voyage-api-key\u001b[39m\u001b[33m\"\u001b[39m, model_name=\u001b[33m\"\u001b[39m\u001b[33mvoyage-2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m sparse_ef = BGEM3EmbeddingFunction()\n\u001b[32m      3\u001b[39m cohere_rf = CohereRerankFunction(api_key=\u001b[33m\"\u001b[39m\u001b[33myour-cohere-api-key\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'VoyageEmbeddingFunction' is not defined"
     ]
    }
   ],
   "source": [
    "dense_ef = VoyageEmbeddingFunction(api_key=\"your-voyage-api-key\", model_name=\"voyage-2\")\n",
    "sparse_ef = BGEM3EmbeddingFunction()\n",
    "cohere_rf = CohereRerankFunction(api_key=\"your-cohere-api-key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f2efd",
   "metadata": {},
   "source": [
    "### Experiment I: Standard Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4349380a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf7c5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dense_ef' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m standard_retriever = MilvusContextualRetriever(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     uri=\u001b[33m\"\u001b[39m\u001b[33mstandard.db\u001b[39m\u001b[33m\"\u001b[39m, collection_name=\u001b[33m\"\u001b[39m\u001b[33mstandard\u001b[39m\u001b[33m\"\u001b[39m, dense_embedding_function=\u001b[43mdense_ef\u001b[49m\n\u001b[32m      3\u001b[39m )\n\u001b[32m      5\u001b[39m standard_retriever.build_collection()\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m dataset:\n",
      "\u001b[31mNameError\u001b[39m: name 'dense_ef' is not defined"
     ]
    }
   ],
   "source": [
    "standard_retriever = MilvusContextualRetriever(\n",
    "    uri=\"standard.db\", collection_name=\"standard\", dense_embedding_function=dense_ef\n",
    ")\n",
    "\n",
    "standard_retriever.build_collection()\n",
    "for doc in dataset:\n",
    "    doc_content = doc[\"content\"]\n",
    "    for chunk in doc[\"chunks\"]:\n",
    "        metadata = {\n",
    "            \"doc_id\": doc[\"doc_id\"],\n",
    "            \"original_uuid\": doc[\"original_uuid\"],\n",
    "            \"chunk_id\": chunk[\"chunk_id\"],\n",
    "            \"original_index\": chunk[\"original_index\"],\n",
    "            \"content\": chunk[\"content\"],\n",
    "        }\n",
    "        chunk_content = chunk[\"content\"]\n",
    "        standard_retriever.insert_data(chunk_content, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561233df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
